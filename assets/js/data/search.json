[ { "title": "The Next Generation of Sensitive Data Detection", "url": "/posts/the-next-generation-of-sensitive-information-detection/", "categories": "Cyber Security", "tags": "AI, LLM, data_classification, data_security, sensitive_information_detection, sensitive_information, BERT", "date": "2024-12-01 00:00:00 +0200", "snippet": "About 6 months ago, I resigned from my comfortable Engineering Manager position to explore new opportunities. After some rest and vacation time, I wondered what to do next.I have always been drawn ...", "content": "About 6 months ago, I resigned from my comfortable Engineering Manager position to explore new opportunities. After some rest and vacation time, I wondered what to do next.I have always been drawn to indie hacking, so I started brainstorming ideas - “Hey, I’ve spent years applying machine learning to catch cyber bad guys… what if I could turn that experience into my own product?”I wasn’t interested in the traditional startup path of raising VC funding, building a large team, and working endless hours. Instead, I wanted to stay small and build a bootstrapped product I could sell to a focused customer base.After doing some market research (serious Googling and coffee chats with potential customers), I embarked on a journey with a friend and ex-colleague to build a Sensitive Data Detection SDK - a library that scans text or files to find potentially sensitive data.In this blog post, I’ll explain the world of sensitive information detection, why it is a problem, what are the current solutions, and how AI is going to revolutionize this field.The Data Security ProblemToday’s businesses store enormous amounts of data. Some of the data can be considered “classified”: user’s personal identifiers, financial data, legal information, intellectual property, and even medical information.Even if an organization doesn’t care about protecting all of this data, they have to - this data is also subject to different regulation policies - like GDPR, HIPAA, and more.Now the organization’s goal is to prevent the leakage of its sensitive data. Putting aside how to monitor data and prevent its leakage, just finding the sensitive data is harder than it sounds.Why Is Data Security HardSensitive data comes in many forms - from structured data like social security numbers and credit card numbers to unstructured content like medical conditions and legal agreements. This variety requires deep understanding of security risks and complex definitions of sensitive entities.Adding to the challenge, sensitive information isn’t centralized - it’s scattered across databases, documents, and emails. Each organization also has unique security needs, like Walmart considering customer IDs as sensitive information.Current Solutions Are Not EnoughThe dominant data security products in late 2024 are just falling short of protecting their customers.The technology is outdated - they use regex patterns to find common sensitive information formats and then run basic validation algorithms to reduce false positives. Even the more advanced solutions that examine context merely search for predefined keywords around detected entities.This is why customers are ending up with alert fatigue, stop looking at alerts, and miss real breaches.Case StudyTesting an existing solution on random documents revealed numerous false positives, including this example from a PDF with a False-Positive detection of a credit card: The number 9783641027629 was flagged as a credit card. It passed Luhn checksum validation. The publication date was mistaken for an expiration date.While the pattern matching worked, the solution failed to understand that this was clearly book metadata, not payment information. Humans can tell the difference because we understand the context.Data Security Next-GenWe reached a conclusion that simple pattern-based search results are… well, embarrassing.Think about it - to tell the difference between a credit card and a book ISBN, we humans naturally understand the context around these numbers. But how do we make a computer understand the semantic meaning of text?Enter AI models, once again stealing the show.Classification modelsWhile Large Language Models are making headlines for their text understanding capabilities, they’re not always the best tool for the job. For sensitive data detection, encoder-based models like BERT excel at creating compact semantic representations - perfect for classification tasks.These models are not just more efficient to run, they’re specifically designed for classification with dedicated output layers that provide clean, consistent predictions. Unlike LLMs that generate free-form text, they output a single class from a predefined set of categories.Building a sensitive information classifier requires carefully curated training data. Since sensitive information isn’t typically published on kaggle, we need to work harder: scraping available data, faking some, or generating it using an LLM.With this dataset, we can fine-tune a pre-trained BERT model to create a robust classifier specifically optimized for detecting sensitive information.The Funnel ApproachThe next generation of data security requires another step of AI-powered filtering on top of traditional pattern matching and validation.This final layer uses an AI classification model that predicts whether the surrounding text context matches the type of suspected sensitive entity.Document ClassificationBeyond verifying individual entities, we can classify entire documents to better understand the context. Is it a financial transaction, company policy, legal agreement, or CV? This classification adds confidence to our detection.For example, an International Banking Account Number (IBAN) in a financial document will be flagged as sensitive, but an IBAN inside a charity flyer requesting donations to be wired to this IBAN is actually intended to be public and should not trigger an alert.Smarter PoliciesThe way companies customize data security products to their use cases is through policies. The user chooses what sensitive information types they want to enable in a policy, and what severity to assign to alerts to signify importance and mitigation priority.If each detection result also includes the classification of the document it was found in, users can create policies that take this into consideration, triggering alerts on entities only for certain document types or adjusting alert severity accordingly.Customer-Specific ModelsIn my vision, the next phase of embedding AI in data security is fine-tuning the models on the customer’s data.While our base models are trained on diverse data, they can be fine-tuned for each customer’s use case: the customer’s data will be scanned once and labeled. Then, the model will be fined-tuned on this data, and lastly, the trained model will be deployed to the customer.Furthermore, the model can be fed with data from false detections and relearn their correct classification to avoid alerting again on similar content.ConclusionI predict that soon enough, during 2025, solutions will start introducing the AI-enhanced detections, the alerts they raise will get more relevant, and customers will start trusting them again. Vendors that will not do it, will stay behind and slowly disappear.This rundown should give you an intro to sensitive data detection in the data security world. I hope you enjoyed! :)Further reading To further understand how sensitive information types are commonly defined, you can check out Microsoft’s published Sensitive information types. I found Google DLP almost unusable due to the volume of spam it produces - this blog post shows it . Some vendors are already talking about advancing sensitive data classification in the AI age." }, { "title": "The AI Trend - LLMs and Generative AI for Everyone", "url": "/posts/the-ai-trend/", "categories": "Machine Learning", "tags": "Machine Learning, AI, LLM, Generative, trend", "date": "2023-06-19 15:21:20 +0300", "snippet": "We are just in the middle of 2023, but it will be known as the year were the mass public has begun accessing and using AI. Whether or not AI will take over a lot of aspects of our life or fall into...", "content": "We are just in the middle of 2023, but it will be known as the year were the mass public has begun accessing and using AI. Whether or not AI will take over a lot of aspects of our life or fall into a more proportional and practical usage is yet to be known.In this post I’ll try to explain what wheels are turning in the tech industry that drives this revolution and how to jump on this LLM train as well.How the trend begunSo what started it? ChatGPT was released on November 30, 2022. This is “AI” search popularity on google trends:Even though AI tech started gaining popularity about a decade ago, ChatGPT was the first product that enabled access to Artificial Intelligence for the mass public. Students began using it for their homework, content writers get its help when writing any piece of long text, and many many more people that are not in tech are now using it.Other companions to chat AI applications are art-generating apps like DALLE-2 (also from OpenAI, the same creators as ChatGPT), and code-generating apps like GitHub Copilot.The hype is about Genrative LLMsChatGPT isn’t just an “AI”. Artificial Intelligence is a broad term that includes different types of technologies. Specifically, ChatGPT uses a Generative-AI algorithm and an LLM - a Large Language Model. It is under the Machine Learning algorithms umbrella and specifically the Deep Learning (or Neural Networks) models family.Generative AI means a model is trained to complete the user - the user supplies a prompt, and the algorithm tries to guess the rest. If a user gives a question as a prompt - the model can suggest an answer as a natural continuation of the prompt.LLMs are just referring to the data type of the content - language. The user “talks” with the model using a natural language, and that is what makes it so compelling to use by everyone - all of us can use a language, and then, suddenly, no training is required to use AI technology.The first ‘L’ in LLM refers to the size of the model - they are Large. When compared to previous models, ChatGPT made a huge increase in inputs. It was trained on 45TB of data and has 1.5 Billion parameters!The revolution leadersOther tech companies have still been developing and using AI before OpenAI - big tech companies like Amazon, Meta, and Google had been working on their own model, and the hype just pushed them to release their models sooner.There are also open-source models getting released frequently on Hugging Face that offer powerful LLMs for everyone to use.All of them are creating and using cutting-edge LLMs and driving the Generative AI revolutionThe followersA lot of tech companies that aren’t AI-driven watched the news about AI. Everyone starts talking about it and discussing how the future is going to change by it.Therefore, a lot of SaaS products are now getting some AI features embedded in them.In my personal opinion, using AI doesn’t always add additional value to the world, but nonetheless, companies are afraid to miss the train so they piece together AI solutions with their current offering, even if the pieces don’t fit.AI for everyoneBut how do they do it? The big tech companies provide simple API’s so every engineer can access them and use AI in their application. Let’s explore the main concepts to know when trying to integrate to those APIsEmbeddingsEven though we talk to AI chats using natural language, it is still a computer, and computers need numbers to operate, not words. Embeddings are just a numerical representation of the text. The input text goes through tokenization to divide a sequence of text into smaller units, or tokens, such as words, sub-words, phrases, or characters. Then the tokens are passed through the model’s “brain”, and it creates an embedding of the given input.A good model tries to keep the text’s semantical meaning inside those numbers - the intention of the writer will be kept in a numerical representation. This is where the Intelligence of those models lies.SimilarityOnce the model creates an embedding, we can see it as a vector. We can observe that if the model does its job well, 2 semantically-similar sentences will produce 2 nearing vectors. Common algorithms to measure vector closeness or similarity are Euclidean distance, Cosine, or Dot Product.Vector DBThe latest buzz in using AI is Vector DBs. If embeddings can create meaningful representations of data, as a user, I want to store representations of my own data. Vector DB gives the ability to store embeddings produced by AI models and search for embeddings similar to them later on, using some similarity search algorithm.ExampleLet’s say we are IKEA customer service engineers, and we want to help our buyers to build their furniture. Really not IKEA style, right?We want to create a chat where a customer describes where he is stuck in the assembly process, and the chat will answer him on how he should proceed.We decide to use one of the OpenAI models.In advance, we collect all the text knowledge ikea has - assembly manuals, all Q&amp;A documents, and previous human customer service conversation transcripts. We query the OpenAI embeddings API to create embeddings from all the knowledge we collected, and we store all the vectors from the responses together with the original texts in milvus vector DB.Now we write the app - a chat server that receives text as input. The app will take the input text and query OpenAI API to get an embedding from this text. The app will search the vector DB for the top 3 embeddings similar in semantic meaning that might contain the answer to the question. The app will take the correlating text and display to the user all of them as optional solutions to the user’s problemAnd Voila - we embedded AI in IKEA 🙂Let’s hope the model works well, and no one will hammer together his nightstand drawers because the chat bot suggested it.Here is some reference code with a different example." }, { "title": "Creating Real-World AI Models by Newbies With ChatGPT", "url": "/posts/coordinates-model-with-ChatGPT/", "categories": "Machine Learning", "tags": "Machine Learning, AI, ChatGPT, geo-location, model, k-NN, coordinates, geonames, newbies", "date": "2023-01-28 11:21:07 +0200", "snippet": "After ChatGPT came out, I challenged it greatly - with riddles, general questions, and lots of code-generation tasks.It stood firm for most of my dares, and even when it didn’t, it still pretended ...", "content": "After ChatGPT came out, I challenged it greatly - with riddles, general questions, and lots of code-generation tasks.It stood firm for most of my dares, and even when it didn’t, it still pretended to know the answer and provided a detailed wrong explanation, the same behavior as many of my coworkers.After a while, it became a major tool in my arsenal, to the extent that whenever I open a new tab in chrome and type ‘c’, the browser suggests chat.openai.com/chat even before calendar.google.com/...The TaskEven though I’m not a data scientist, I have had some experience training basic machine learning models before. This time, I wanted to get into the shoes of a machine learning newbie - with only basic Machine learning knowledge and some python pandas experience.The goal is to see how ChatGPT can create a real Machine Learning model from scratch - with as little intervention on my side as possible, especially in the data collection process (known as the most challenging part of training usually).I wanted to find a fun challenge with enough data online, but the data shouldn’t be machine-learning-ready. So I started browsing some GitHub awesome lists to get inspiration. When I stumbled upon multiple geolocation projects, I got an idea: Train a model to predict in which country a certain coordinate is!Hey ChatGPTLet’s fire up that chatGPT and ask him for help! Hey, chatGPT, I want to create a small AI program, and I don’t want to work hard on my own. I want to write a small program that receives two coordinates as input and predicts in which country they are. Are you up for the challenge? Yes, I can help you create a small AI program that predicts the country based on given coordinates. However, keep in mind that this task would require a dataset of coordinates and corresponding countries. Without this dataset, the model cannot be trained. Additionally, it would be helpful if you could provide more details about the specific requirements and constraints of the program.It is up for the challenge and understands the mission before us well enough!The DatasetHopefully, a dataset will be available and easily downloadable. Which dataset can I use? There are a few datasets that you can use for this task: GeoNames dataset: This dataset provides information about various geographical features such as cities, countries, and administrative divisions. It includes information such as coordinates, population, and time zone. The dataset can be found here: http://download.geonames.org/export/dump/ Natural Earth dataset: This dataset provides detailed information about the world’s countries, including their boundaries, coastlines, and other geographical features. The dataset includes information such as coordinates, population, and GDP. The dataset can be found here: http://www.naturalearthdata.com/downloads/ Open Street Map dataset: This dataset is a collaborative project to create a free editable map of the world. It includes information such as coordinates, roads, buildings, and land use. The dataset can be found here: https://www.openstreetmap.org/export#map=5/51.500/-0.100 You can use the above datasets or others that you might find, but make sure the dataset includes the information you need to train your model.Cool, from looking briefly at the three options, Natural Earth dataset looks like a win because it has border coordinates, but from a glance in the downloads section, it isn’t the easiest choice: the data isn’t in a simple CSV format, and it will be another challenge to explain to the model the concept of a “border” that separates labels.For simplicity, we prefer a dataset with multiple coordinates and the corresponding country in order for the model to learn something like “Oh, I see that those coordinates are mathematically near each other, and they are all called country X, so probably another close coordinate will also be in country X.” So let’s just say that in the newbies’ shoes, we chose the first option, as an unknowledgeable newbie may do. Okay, let’s try the first dataset! I want to download allCountries.zip, and create a DataFrame with latitude, longitude, and country columns. Sure, you can download the allCountries.zip file from the GeoNames website, and then extract the data to a csv file. Once you have the data in a csv file, you can use a library such as Pandas to create a DataFrame. Here is some sample code to get you started:import pandas as pd# read the data from the csv filedata = pd.read_csv(\"allCountries.txt\", sep='\\t', header=None, names=[\"country\",\"name\",\"asciiname\",\"alternatenames\",\"latitude\",\"longitude\",\"feature_class\",\"feature_code\",\"country_code\",\"cc2\",\"admin1_code\",\"admin2_code\",\"admin3_code\",\"admin4_code\",\"population\",\"elevation\",\"dem\",\"timezone\",\"modification_date\"]) # select the columns of interestdata = data[[\"latitude\",\"longitude\",\"country\"]]# display the first 5 rows of the DataFrameprint(data.head())… (Explanation redacted) DataFrame is a commonly used data structure in data science. In such projects, it is also comfortable to work inside some python notebook - a web wrapper around a python interpreter that lets you execute code snippets and visualize the output nicely.I’ll fire up a jupyter notebook, and execute the code in a cell. This is the output: latitude longitude country0 42.64991 1.53335 29860431 42.58765 1.74028 29947012 42.61203 1.47364 30076833 42.52535 1.73343 30178324 42.52915 1.73362 3017833It worked!The latitude and longitude look good, but the country is wrong.According to the readme.txt under the GeoNames link, the first column in the CSV is actually “geonameid” and not “country”. We will let this small mistake pass by and modify the read_csv function call to use “country_code” instead, as “country”.data = pd.read_csv(\"allCountries.txt\", sep='\\t', header=None, names=[\"geonameid\",\"name\",\"asciiname\",\"alternatenames\",\"latitude\",\"longitude\",\"feature_class\",\"feature_code\",\"country\",\"cc2\",\"admin1_code\",\"admin2_code\",\"admin3_code\",\"admin4_code\",\"population\",\"elevation\",\"dem\",\"timezone\",\"modification_date\"])After re-executing the code, we got the following: latitude longitude country0 42.64991 1.53335 AD1 42.58765 1.74028 AD2 42.61203 1.47364 AD3 42.52535 1.73343 AD4 42.52915 1.73362 ADAnd it works!AD country code stands for Andora, but when dropping the coordinates in Google Maps, we can see that some are indeed in Andora, but others are sliding over the border into France.I used data.sample(10) to check other coordinates further, and they were correctly labeled.Generally, the data is accurate with small around-the-border errors, but we will take the errors into consideration later.Model TrainingLet’s create some Artificial Intelligence! Now, please write a python script to create a prediction model from the DataFrame above! Sure, here is some sample code to create a simple prediction model using the latitude and longitude as input and the country as the output:from sklearn.model_selection import train_test_splitfrom sklearn.neighbors import KNeighborsClassifier# Split the data into training and test setsX = data[[\"latitude\", \"longitude\"]]y = data[\"country\"]X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)# Create a KNN classifier with 5 nearest neighborsknn = KNeighborsClassifier(n_neighbors=5)# Fit the classifier to the training dataknn.fit(X_train, y_train)# Print the accuracy of the model on the test dataprint(\"Accuracy:\", knn.score(X_test, y_test)) This code will use K-Nearest Neighbors (KNN) algorithm to predict the country based on the given coordinates. The KNN algorithm is a simple algorithm that classifies an input based on the majority class of its k nearest neighbors.In this example, the data is split into a training set and a test set with a 80-20 ratio. Then the KNN classifier is trained with the training set and the accuracy of the model is measured with the test set.Keep in mind that this is a simple example and the accuracy of the model might not be high. You can try experimenting with different algorithms and different values for k to see if you can improve the accuracy of the model.…The code is pretty standard, and the model ChatGPT chose for the task is K-NN, which, as explained, will try to predict a coordinate based on neighbors. If we stretch the truth a little, we can say that the country will be predicted based on “closeness” to other cities - if the 5 closest cities of a given location are all in Japan, the location is probably in Japan as well. Sounds very logical and fit for our task 😁Executing the above throws an exception ValueError: Input contains NaN. I asked ChatGPT if it understood the error, and of course, it provided a long and exhaustive response. To keep this long post a little shorter, I’ll TL;DR:There were some empty values in the DataFrame. We can either try to fill or ignore them. As filling them out takes more work, we will check out how much data we will lose:print(data.shape)print(data.isnull().sum())(12363388, 3)latitude 0longitude 0country 14551There are 12363388 countries in the dataset, from which 14551 are missing. It is only 0.1% of the data. Therefore we will just discard the rows with empty countries:data = data.dropna()Re-executing the K-NN training now succeeds; what accuracy do we get?Accuracy: 0.9971058010307041Amazing! 99.7% accuracy on the first try!Re-evaluationThe astonishing first-try accuracy metric isn’t satisfying enough, for a couple of reasons: The data may be unequally distributed across countries, and accuracy can be misleading in imbalanced data. Take for example a data set with 99% of it in Andora. The model can always locate a given set of coordinates in Andora, and it will be 99% correct. Earlier we saw some errors in the data. GeoNames sources are based on cities/other known locations, so the dataset probably has some level of prejudice.Therefore I want to re-evaluate the accuracy with an external and random dataset. Okay, now let’s create another dataset to further evaluate the accuracy of the KNN model. Let’s select 10,000 random coordinates on a map and send them to some geolocation REST-API. Please write a python script for the task. Use tqdm to display progress and use try-except around fail-prone parts. Sure, here’s some sample code that generates 10,000 random coordinates and sends them to a geolocation API to get the country information:import randomimport requestsfrom tqdm import tqdm# Set the number of coordinates to generatenum_coordinates = 10000# Create an empty list to store the resultsresults = []# Use tqdm to display progressfor i in tqdm(range(num_coordinates)): # Generate a random latitude and longitude lat = random.uniform(-90, 90) lon = random.uniform(-180, 180) # Send a request to the geolocation API try: response = requests.get(f\"https://api.opencagedata.com/geocode/v1/json?q={lat}+{lon}&amp;key=YOUR_API_KEY\") data = response.json() results.append((lat, lon, data['results'][0]['components']['country'])) except: results.append((lat, lon, 'NA'))# Create a DataFrame from the resultsimport pandas as pddf_new = pd.DataFrame(results, columns=[\"latitude\", \"longitude\", \"country\"]) In this code, I am using the OpenCage Geocoding API to get the country information from the coordinates. You need to replace YOUR_API_KEY with your API key to get access to the API. You can sign up for a free API key on the OpenCage websiteI followed the instructions, and modified the code with the correct API_KEY, num_coordinates to 2500 because this is the amount of free API requests toOpenCage, and saved the result to csv.A sample of the result: latitude longitude country80 -67.057900 151.629167 NA694 61.930620 -66.216010 Canada1667 7.481893 -155.699348 NA1669 -70.585375 -24.491804 NA2203 70.407657 28.774544 NorwayAs we can see, there are a lot of “NA” errors - we are left with 30% of the data. The world is really mostly water.Also, the country is a full name, but the original dataset has a country code, so with the help of chatGPT and some small modifications, I translated the country to country code using countryInfo.txt, and run the predication on the newly-collected data:from sklearn.metrics import accuracy_score# predict the country of the coordinates in the new dataframepredictions = knn.predict(df_new[[\"latitude\", \"longitude\"]])# calculate the accuracy of the model on the new dataframeaccuracy = accuracy_score(df_new[\"country\"], predictions)print(f\"Model accuracy: {accuracy}\")The result is:Model accuracy: 0.986764705882353.Amazing! A little worse than the previous 99.7% precision, but still a staggering accuracy for so little effort put into the model.Exploring the misses is interesting:df_new[\"predictions\"] = predictionsprint(df_new.loc[df_new[\"country\"] != df_new[\"predictions\"]]) latitude longitude country predictions336 -18.241242 16.706105 NA AO931 -25.104256 22.713657 BW ZA958 -23.995538 17.280858 NA ZA1348 -9.672760 -138.965349 FR PF1457 -25.492231 16.244680 NA ZA1674 25.880065 -14.315396 MA EH1991 10.419924 23.470560 SD CF1999 -49.213029 -73.137265 AR CL2160 -22.501809 16.604622 NA BWThey are mostly near the borders of African countries, where the original dataset might have some errors or lacks data 😄Another error is somewhat funny and political: French Polynesia is a country in GeoNames, but it is classified as France in OpenCage.ConclusionChatGPT is a tremendous technology. With little input and just small modifications, we created a highly accurate machine-learning program that predicts real-world geolocations.Here is the jupyter notebook and the full conversation.Disclosure: to be honest, this is my second conversation for the same task. The first time I talked with ChatGPT, I tried a smaller dataset from the same source (cities500.zip). More dataset corrections were needed, and the first model training code it offered was LogisticRegression, which resulted in 51% accuracy.As a result, ChatGPT offered to try different “solvers” (which gave ~65% accuracy), and other algorithms, including Random Forest and K-NN, resulting in 93% and 92% accuracy.* I really considered addressing ChatGPT as “they” instead of “it” in this post" }, { "title": "Big data for Dummy Developers - Step Into Foreign Territory", "url": "/posts/how-to-send-big-data/", "categories": "Big Data", "tags": "Big Data, dummies, Tabular, Partitioning, Consistency, Veracity, json, five V', developer", "date": "2022-12-18 00:23:21 +0200", "snippet": "So it happens that in about the past year and a bit, I repositioned to a Big Data Engineer role in my company without any prior knowledge.I had been writing data pipelines - primarily Spark Applica...", "content": "So it happens that in about the past year and a bit, I repositioned to a Big Data Engineer role in my company without any prior knowledge.I had been writing data pipelines - primarily Spark Applications in Scala, deployed over k8s infrastructure.After dealing with those scary words and some others (Iceberg, Data lake, parquet, Kafka, Helm charts, and more), I can say I have learned a lot, and then I realized something - it’s not just another flavor of backend engineering - but a whole big thing.Lately, I went back to my natural habitat - developing cyber-security solutions in a system environment in c++.And then, my previous realization became even deeper - data is everywhere, and basic knowledge in the field can become handy for any modern software solution in every field.Why data is important?We live in an age where in order to prove something - we need data to back us up. If we want to make a business decision, we need those numbers, dashboards, and graphs.An app developer can claim that he feels the app’s performance is too slow - therefore, improving performance should be the most urgent task to do next. But maybe the app metrics will show it is still below the agreed thresholds, but nevertheless, it crashes once for every 3 instances, which could be revealed if telemetry has been sent.Simply put - in order to plan our software better, make more confident decisions, and just track what happens within our app, we need to become data-driven.While adapting to the new team, the engineering environment, the common jargon, and the technology stack, I couldn’t find any decent Big-Data for Dummy developers TL;DR. So if you are ever going to collect and send custom data in your app, here are a couple of concepts to help you step into this foreign territory.The dummiest exampleLet’s say our app scans files, and we want to send a JSON containing information about the files scanned, such as file format, file size, and scan duration.You decide to send the following JSON:{\t\"timestamp\": \"Fri Dec 16 22:42:50 IST 2022\",\t\"PDF version 1.4\": [\t\t{\t\t\t\"file size\": \"135KB\",\t\t\t\"scan duration\": 531000\t\t},\t\t{\t\t\t\"file size\": 43105,\t\t\t\"scan duration\": 0.81\t\t},\t\t// ...\t],\t// ...}Each JSON has a timestamp and an array of file size and scan duration dictionaries for each file format. Because the scan time should differ across file types, and you support only a small set of file types, you separate the arrays both logically and to send less data.Let’s explore key Big Data concepts through mistakes in the design aboveKey conceptsTabular format❌ Problem - file format is a dynamic key and causes nesting.Doesn’t matter what data we are about to send or which backend services will process and ingest it, strive to format the data as a table - every piece of information should have its own column with the same name.We should think about how the data will be read - usually, it is by SQL or SQL-like querying (it’s true for both good old SQL databases as for new NoSQL Big Data solutions).In key-value formats, logically keys become columns, and therefore the keys should be constant and not based on the data we collect.In the case we send JSONs like this, and the backend ingestion isn’t in our control (different team or 3rd party solution), we can find ourselves with the following table: Timestamp PDF version 1.4 PDF version 1.7 Fri Dec 16 22:42:50 IST 2022 [{“file_size” : …}] null Fri Dec 16 22:42:52 IST 2022 null [{“file_size” : …}] Almost each data query will require some JSON parsing that can slow down the query duration significantly, the columns can grow without a limit, there will be a lot of null fields, and querying a group of file formats will be a complex task (no LIKE statement for column names).PartitioningYou might know what partitioning is from regular SQL databases: it is an efficient method to split the data according to a specific column we often filter by.If we expect to see a daily report on a summary of specific data from our app, the partition column will be a new “day” column which can be derived from the timestamp.In NoSQL or Big-Data solutions, the data is usually stored in files on the disk (or cloud storage) in special formats such as parquet, ORC, and others.To make it easy to partition the data files, the solution is very neat - each partition will be a folder! A new folder will be created for each new day, and when we execute our day-specific query, only the files from that folder will be processed. 🤯scans.db/├── day=2023-01-01│   ├── part1.parquet│   ├── part2.parquet│   ├── part3.parquet│   └── part4.parquet├── day=2023-01-02│   ├── part1.parquet│   ├── part2.parquet│   └── part3.parquet└── day=2023-01-03 ├── part1.parquet └── part2.parquet✅ Improvement: Save the timestamp in an easier format to parse and extract the day easier, such as YYYY-MM-DD hh:mm:ss.But you can, and sometimes should, partition by more than one column.In our example, there will be times when we will want to query and make observations about one specific file type only, hence we should partition by it. ✅ Improvement: Because we want to know about the file type without filtering a specific version, we can even improve our end query by sending the version and other data in a different field, and the file type field will hold “PDF” or “DOC” only. This way, we can be more confident that we will hit our partition without using LIKE statements as was needed before.Consistency❌ Problems - file size values have different types, and scan duration values have different units.We want to normalize and keep our data consistent from the sending point - handling it later in the ingestion or during the query can cause mistakes and confusion. ✅  Decision breaker: If it’s not essential for the app, prefer using the smallest unit when possible - microseconds instead of seconds as scan duration, and bytes instead of kilobytes for file size. Some reasons are that floating points can cause arithmetic mistakes, and integers will take less space than floats if you use a protocol such as protobuf. Only if the smallest unit can expose int overflow, use the smallest unit that doesn’t exceed an int size.Veracity - or send more, not lessThis one I actually stole from Big Data five V’s, and I refer to an aspect of this concept that can be summarized by a sentence I heard from a co-worker: “I won’t send this string because I can later on correlate it from another data source, and in the worst case, I’ll manage without this info”.Sometimes it’s good to leave out data you don’t need. Still, lots of times, we can be very sorry that after all this hard work to create a whole data pipeline, we end up without all the information we want because we tried to save a string or two, especially if we are below pre-defined size limits.We should add any useful data we can for further analysis and diagnosis. We can also include in our scan telemetry information like app version and Git hash of the deployed app, so we can identify bugs and track that they are solved in newer versions.Other information we can later use is the unique id of the app, the name, and the hash of the file - so in case we find anomalies in the scanning time, we can later contact the client and ask to get the file for bug reproduction.Let’s create a new JSON according to the suggested changes so far:{\t\"timestamp\": \"2022-16-12 22:42:50\",\t\"git hash\": \"f1da229780e39ff6511c0fc227744b2817d122f4\",\t\"version\": \"5.1.2.2\",\t\"file type\": \"PDF\",\t\"file size\": 924850,\t\"scan duration\": 445000,\t\"uid\": \"67a8eb1f-b838-4cf7-9f3e-4658f909ca3b\",\t\"file hash\": \"f8c3f233bc8e7f64c3a237f7c877323c16916919\",\t\"file location\": \"/home/lior/big_data_for_dummy_developers.pdf\",}That’s it for this blog post, and part 2 is coming with more big data at a glance.Hope you learned something new!" }, { "title": "My Simple Utility for Kernel Function Graph Tracing", "url": "/posts/kernel_tracer/", "categories": "Linux", "tags": "Linux, Tracing, strace, trace-cmd, utilty", "date": "2022-01-10 20:39:22 +0200", "snippet": "Have you ever wondered how come strace is such a popular tool, while all it shows you are only the syscalls a program is calling and nothing more?Well, in most cases it’s enough. But sometimes, eve...", "content": "Have you ever wondered how come strace is such a popular tool, while all it shows you are only the syscalls a program is calling and nothing more?Well, in most cases it’s enough. But sometimes, even rarely, a programmer will also want to inspect what happens under the kernel’s hood inside the syscalls.I have been investigating kernel behavior on some basic tasks, like module loading for example, so my mission was to figure out the code path that is taken inside the kernel upon a simple invocation of the insmod command.strace wasn’t very helpful: it only displayed a call for an init_module syscall, but I wanted the function graph inside it!After some web searches, and I didn’t find any simple tool fitted to my problem.The closest thing was trace-cmd, which uses the ftrace API for the job and creates a function graph as I wish. But I need to run it in a separate shell from my traced program, and also pass it my program’s PID, which is problematic for quickly exiting simple utilities like insmod.As a result, and as any decent bored engineer would do, I created a wrapper utility to exactly fit my needs 😁Welcome kernel_function_trace !bad name, but ktrace was too presumptuous.Shortly, it’s 50 lines of a C-code wrapper around trace-cmd, with some forking and signaling (and a tiny bit of sleeping), that makes kernel function tracing a little bit easierIt wraps around any program the same way strace does:./kernel_function_trace insmod my_kernel_module.koAnd saves the long function call graph to a file, which can be pretty-printed with trace-cmd report :root@ubuntu:~/work# trace-cmd report | grep -A 15 load_module insmod-18954 [002] 30213.994129: funcgraph_entry: | load_module() { insmod-18954 [002] 30213.994130: funcgraph_entry: 0.593 us | find_sec(); insmod-18954 [002] 30213.994131: funcgraph_entry: 0.681 us | get_next_modinfo(); insmod-18954 [002] 30213.994132: funcgraph_entry: 0.480 us | find_sec(); insmod-18954 [002] 30213.994132: funcgraph_entry: 0.497 us | find_sec(); insmod-18954 [002] 30213.994133: funcgraph_entry: 0.490 us | find_sec(); insmod-18954 [002] 30213.994134: funcgraph_entry: | mod_verify_sig() { insmod-18954 [002] 30213.994134: funcgraph_entry: 0.213 us | mod_check_sig(); insmod-18954 [002] 30213.994134: funcgraph_entry: | verify_pkcs7_signature() { insmod-18954 [002] 30213.994135: funcgraph_entry: | pkcs7_parse_message() { insmod-18954 [002] 30213.994135: funcgraph_entry: | kmem_cache_alloc_trace() { insmod-18954 [002] 30213.994135: funcgraph_entry: | _cond_resched() { insmod-18954 [002] 30213.994135: funcgraph_entry: 0.202 us | rcu_all_qs(); insmod-18954 [002] 30213.994136: funcgraph_exit: 0.571 us | } insmod-18954 [002] 30213.994136: funcgraph_entry: 0.202 us | should_failslab(); insmod-18954 [002] 30213.994136: funcgraph_entry: 0.294 us | memcg_kmem_put_cache();Voilà!You can find it together with a fair README on my GitHub page:https://github.com/xrl1/kernel_function_traceEnjoy!" }, { "title": "Leveraging LD_AUDIT to Beat the Traditional Linux Library Preloading Technique - repost", "url": "/posts/Leveraging-LD_AUDIT/", "categories": "Linux", "tags": "Linux, LD_AUDIT, LD_PRELOAD, evasion, library hijacking, preloading, ld.so", "date": "2022-01-05 01:22:20 +0200", "snippet": "About a year ago I while I was going through the code of the standard library loader, ld.so, I encountered an interesting auditing API.Soon I found out that this API is very handy and powerful - on...", "content": "About a year ago I while I was going through the code of the standard library loader, ld.so, I encountered an interesting auditing API.Soon I found out that this API is very handy and powerful - only by setting an environment variable named LD_AUDIT, it will load my own library to a process in a very early stage in its initialization.This was genuinely fascinating for me because up until then, the most commonly used technique was using LD_PRELOAD for library preloading, and LD_AUDIT was unknownI managed to both create a rootkit - by hijacking library calls, and also defend against LD_PRELOAD - by blocking its loadingAs those are unheard-of techniques in the Linux cyber-security community, I wrote a blog post about it through my employer, SentinelOne, and you can read all about it there 🙂linkEnjoy!External links and further reading on the topic:LD_PRELOAD trick:https://www.goldsborough.me/c/low-level/kernel/2016/08/29/16-48-53-the_-ld_preload-_trick/The man page for the loader’s auditing API:https://man7.org/linux/man-pages/man7/rtld-audit.7.htmlThe loader’s source code is inside glibc’s repository:https://code.woboq.org/userspace/glibc/The most relevant files are rtld.c and all dl-*.cLibprocesshider repository:https://github.com/gianlucaborello/libprocesshiderMore on process hiding and LD_PRELOAD:https://sysdig.com/blog/hiding-linux-processes-for-fun-and-profit/Thorough explanation on symbol resolving: https://ypl.coffee/dl-resolve/MITRE’s technique on LD_PRELOAD:https://attack.mitre.org/techniques/T1574/006/Patch the loader to disable preloading: https://github.com/hc0d3r/ldpreload-disablelibpreloadvaccine repository and article:https://github.com/ForensicITGuy/libpreloadvaccinehttps://medium.com/forensicitguy/whitelisting-ld-preload-for-fun-and-no-profit-98dfea740b9" }, { "title": "My First Commit in the Linux Kernel - Patching a Bug in the binfmt Kernel Component", "url": "/posts/kernel-patch/", "categories": "Linux", "tags": "Linux, kernel, binfmt_misc, Submit Patch", "date": "2022-01-02 23:31:28 +0200", "snippet": "Earlier in 2021, about March, my own code got into the Linux kernel!Long story short, I found a bug in some binary execution handling component called binfmt_misc, got to the bottom of it and fixed...", "content": "Earlier in 2021, about March, my own code got into the Linux kernel!Long story short, I found a bug in some binary execution handling component called binfmt_misc, got to the bottom of it and fixed it, suggested a patch, and it got applied 🙂You can check out the commit here:https://github.com/torvalds/linux/commit/e7850f4d844e0acfac7e570af611d89deade3146As a big Linux enthusiast, you can just imagine how excited I’ve been when I saw this:Now I can retire in peace.The long storyI have been working on an educational Linux challenge, and I was trying to understand better the kernel component binfmt_misc for a specific level I had in mind.For a brief explanation on the component, here is its Wikipedia page:“ binfmt_misc (Miscellaneous Binary Format) is a capability of the Linux kernel which allows arbitrary executable file formats to be recognized and passed to certain user space applications, such as emulators and virtual machines.[1] It is one of a number of binary format handlers in the kernel that are involved in preparing a user-space program to run.“So this component allows any program to register a custom binary handler, according to some file name extension or file header magic the program defines.Then, upon the execution of the binary, the binfmt_misc component will identify the defined program and will pass it the handling of the execution, instead of the more commonly known ELF handler or bash script handler.Ok, so I was messing around with registering a new binary handler, by echoing some input into a file named register in the binfmt filesystem but for some reason.. the process was stuck and didn’t return. I tried to terminate it brutally with “kill -9”, but to my surprise, it didn’t respond! A system reboot was necessary to recover this process 😛This is my input in bash:echo \":iiiii:E::ii::/proc/sys/fs/binfmt_misc/bla:F\" &gt; /proc/sys/fs/binfmt_misc/registerAs you can see, the kill signal is sent successfully, but no response from bash (from which I executed the ‘echo’ builtin command)The immortal bashAlrighttt, some interesting behavior from the OS, don’t you think?Let’s get digging/proc/sys/fs/binfmt_misc/register isn’t a standard file in the filesystem.It is actually a mount point (mounted by default by many standard Linux distributions):binfmt mount pointThen binfmt_misc is also.. a filesystem! Or more accurately, a pseudo-filesystem, which implements an interface to communicate with the binfmt kernel component from user-space 🙂Ok, so what is the register file, and what is the string I just echoed means?From a documentation written by kernel.org:“To actually register a new binary type, you have to set up a string looking like `:name:type:offset:magic:mask:interpreter:flags` (where you can choose the `:` upon your needs) and echo it to `/proc/sys/fs/binfmt_misc/register`.”In my input to the file, the name, type, offset, magic, and mask are non-important, therefore we will take a look at how the interpreter – “bla”, a non-existent file in the pseudo-filesystem directory, and flags – ‘F’ (fix binary, more on that later) are all together causing the process freezeGlasses on – reading the kernel codeThe code behind the binfmt_misc filesystem resides in the kernel source code under “fs/binfmt_misc.c”. We can manually locate it because “fs” is the folder where most filesystem code resides 😀Then, we can find the file_operations struct that is assigned to the file “register”:static const struct file_operations bm_register_operations = { .write = bm_register_write, .llseek = noop_llseek,};So according to the above struct, each time a “write” operation is called on the file, the function bm_register_write will be called.After reading the function’s code, there is nothing unusual at a first glance.Now let’s set up a development environment and spawn a kernel debugger, and find out what happens inside the kernel (there are a lot of great guides on how to accomplish this, I used kdbg on qemu with buildroot).Then we find the task (thread in the kernel) in which bash is stuck, and inspect it’s backtrace:0 schedule () at ./arch/x86/include/asm/current.h:151 0xffffffff81b51237 in rwsem_down_read_slowpath (sem=0xffff888003b202e0, count=&lt;optimized out&gt;, state=state@entry=2) at kernel/locking/rwsem.c:9922 0xffffffff81b5150a in __down_read_common (state=2, sem=&lt;optimized out&gt;) at kernel/locking/rwsem.c:12133 __down_read (sem=&lt;optimized out&gt;) at kernel/locking/rwsem.c:12224 down_read (sem=&lt;optimized out&gt;) at kernel/locking/rwsem.c:13555 0xffffffff811ee22a in inode_lock_shared (inode=&lt;optimized out&gt;) at ./include/linux/fs.h:7836 open_last_lookups (op=0xffffc9000022fe34, file=0xffff888004098600, nd=0xffffc9000022fd10) at fs/namei.c:31777 path_openat (nd=nd@entry=0xffffc9000022fd10, op=op@entry=0xffffc9000022fe34, flags=flags@entry=65) at fs/namei.c:33668 0xffffffff811efe1c in do_filp_open (dfd=&lt;optimized out&gt;, pathname=pathname@entry=0xffff8880031b9000, op=op@entry=0xffffc9000022fe34) at fs/namei.c:33969 0xffffffff811e493f in do_open_execat (fd=fd@entry=-100, name=name@entry=0xffff8880031b9000, flags=&lt;optimized out&gt;, flags@entry=0) at fs/exec.c:91310 0xffffffff811e4a92 in open_exec (name=&lt;optimized out&gt;) at fs/exec.c:94811 0xffffffff8124aa84 in bm_register_write (file=&lt;optimized out&gt;, buffer=&lt;optimized out&gt;, count=19, ppos=&lt;optimized out&gt;) at fs/binfmt_misc.c:68212 0xffffffff811decd2 in vfs_write (file=file@entry=0xffff888004098500, buf=buf@entry=0xa758d0 \":iiiii:E::ii::i:CF\", count=count@entry=19, pos=pos@entry=0xffffc9000022ff10) at fs/read_write.c:60313 0xffffffff811defda in ksys_write (fd=&lt;optimized out&gt;, buf=0xa758d0 \":iiiii:E::ii::i:CF\", count=19) at fs/read_write.c:65814 0xffffffff81b49813 in do_syscall_64 (nr=&lt;optimized out&gt;, regs=0xffffc9000022ff58) at arch/x86/entry/common.c:4615 0xffffffff81c0007c in entry_SYSCALL_64 () at arch/x86/entry/entry_64.S:120We see that indeed the code is stuck in binfmt_register_write (11) when calling the function open_exec (10).Looks like down the road a shared lock is taken, probably on the file passed to open_exec (I verified it later), but the lock can’t be obtained, therefore the process waits.So who takes a write lock in a manner that causes an actual deadlock? We should get back to bm_register_write (only relevant parts):static ssize_t bm_register_write(struct file *file, const char __user *buffer, size_t count, loff_t *ppos){ Node *e; struct inode *inode; struct super_block *sb = file_inode(file)-&gt;i_sb; struct dentry *root = sb-&gt;s_root, *dentry; int err = 0; e = create_entry(buffer, count); (1) ... inode_lock(d_inode(root)); (2) ... if (e-&gt;flags &amp; MISC_FMT_OPEN_FILE) { (3) struct file *f; f = open_exec(e-&gt;interpreter); (4) if (IS_ERR(f)) { err = PTR_ERR(f); pr_notice(\"register: failed to install interpreter file %s\\n\", e-&gt;interpreter); (5) simple_release_fs(&amp;bm_mnt, &amp;entry_count); iput(inode); inode = NULL; goto out2; } e-&gt;interp_file = f; } ... err = 0;out2: dput(dentry);out: (6) inode_unlock(d_inode(root)); ...}In (1), the entry is created, and the flags are populated.In (2), a lock is taken on the whole filesystem root (!!!!). Alright, maybe this is it?In (3) , we check the flags we provided in our inputWe can see in some inner code (called somewhere inside create_entry) that ‘F’ adds the flag MISC_FMT_OPEN_FILE, hence the “if” statement is true and we reach the code in (4), which is the only statement that calls open_exec in our code flow.We call open_exec with an interpreter path inside the filesystem root, and even though the file does not exist, an inner code will try to acquire a read lock, but after a write lock is already locking it, the code will… wait.We can be sure by running dmesg and observing that the message in (5) has never been reached.In (6), the write lock is taken down, which is too late: the task is now waiting inside open_exec and the function will never returnIn short, a write lock is taken on the filesystem root, and with a specific input we cause the same task to try and read in the same place before the lock is taken down, and a deadlock transpiresGloves on – Patching the codeYou can see my final patch at the kernel’s Github, which is generally very simple – I just moved the “if” block before the code locks the entire kernel, and some small variable definition and freeing accordingly.After testing the patch on the qemu environment, I was ready to contribute it to the kernel.Then I did the tiresome work to create the patch conforming to the guidelines and sent it as an email to the Linux Kernel Mailing List, and.. nothing.It was the end of December 2020, and I thought that maybe the holidays are causing a delay, but January came by, and my mail wasn’t even acked by anyone.I sent another email, but still nothing…Each section in the Kernel code has a dedicated maintainer, but the maintainer of binfmt_misc.c just didn’t reply 🤷‍♂️I continued with my life, and then, in February 2021, I noticed that someone else has inserted a commit into binfmt_misc.c !I emailed the guy (‫deller@gmx.de‬‏) personally and asked for help, and he pointed out that this code section isn’t really maintained and I should address a guy named Andrew Morton, who picks up random patches and forwards them to Linus.Deller gave me a small fix to the code and forwarded it to Andrew with an ack.Then after a couple of days, and after hopping through many different git repositories (the Linux kernel code process is very complicated) my code got into the kernel, and acked by the one and only, Linus Torvalds!The public email exchange can be found in the Linux Kernel Mailing List archive starting hereBut retirement is too early, so I hope this is not my last patch 😉" } ]
